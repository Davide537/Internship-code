{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255efc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg \n",
    "# Pkg.instantiate()\n",
    "Pkg.activate(\".\")  # activate the environment for this notebook\n",
    "# load the packages\n",
    "using Flux, Dates, BSON, Plots, Measures, JLD2, Noise, JSON, Random\n",
    "using Printf, Statistics, ProgressMeter, ParameterSchedulers, ArgCheck\n",
    "using ParameterSchedulers: Stateful , Optimisers\n",
    "\n",
    "pyplot()\n",
    "# # optional gpu usage\n",
    "const use_gpu = false\n",
    "if use_gpu\n",
    "    using cuDNN, CUDA\n",
    "    CUDA.allowscalar(false) # disable scalar operations on GPU\n",
    "    if CUDA.functional() # check if CUDA is functional\n",
    "        println(\"CUDA is functional, using GPU\")\n",
    "    else\n",
    "        error(\"CUDA is not functional, using CPU\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ce120d",
   "metadata": {},
   "source": [
    "# TSCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2002b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "function collect_data(data, branch; noise_param = 0.1)   \n",
    "    # branch is either 1 or 2, branch 1 is h0, branch 2 is h_s, u_c \n",
    "    # notice how the branch number is the same as the output size\n",
    "    timex = data[\"times\"][branch]\n",
    "    if branch == 1\n",
    "        q_in = data[\"q_in\"][branch]\n",
    "        q_out = data[\"q_out\"][branch]\n",
    "        ext_forc = [ q_in q_out] \n",
    "        lookback = findfirst(timex.>= 24*365 )\n",
    "    else \n",
    "        wind = data[\"tau\"][branch]\n",
    "        ext_forc = wind \n",
    "        lookback= findfirst(timex .>= 24 ) \n",
    "    end\n",
    "\n",
    "    # sizes\n",
    "    n_times=length(data[\"solution\"][branch])\n",
    "    n_steps=n_times # number of time steps in the data\n",
    "\n",
    "    # Create inputs X and outputs Y\n",
    "    n_in = 3\n",
    "    n_out = branch\n",
    "\n",
    "    X = zeros(Float32,n_in,n_steps)\n",
    "    Y = zeros(Float32,n_out,n_steps)\n",
    "\n",
    "    for t in 1:n_steps\n",
    "        X[1:branch,t] .=  data[\"solution\"][branch][t] .* (1 +  noise_param * randn(Float32) )\n",
    "        X[1+branch:end,t] .=  ext_forc[t]\n",
    "        Y[:,t] .= data[\"solution\"][branch][t]   \n",
    "    end\n",
    "\n",
    "    Xμ= mean(X[:,1:lookback], dims=2); Xσ = std(X[:,1:lookback],  dims=2)\n",
    "    # Normalize data only using past values\n",
    "    X = (X .- Xμ) ./ Xσ    \n",
    "    Y = (Y .- Xμ[1:branch,:]) ./ Xσ[1:branch,:]\n",
    "    return X, Y, timex\n",
    "end\n",
    "\n",
    "\n",
    "function make_rolling_test_splits(time, branch; horizon=30)\n",
    "    # Create splits based on horizon lenght\n",
    "    lookback = branch == 1 ? findfirst(time .>= 24*365 ) : findfirst(time .>= 24 )\n",
    "    splits = []\n",
    "    start = lookback\n",
    "    data_len = size(time,1)\n",
    "    while (start + horizon) <= data_len\n",
    "        train_idx = 1:start\n",
    "        val_idx   = (start+1):(start+horizon)\n",
    "        push!(splits, (train_idx, val_idx))\n",
    "        start += horizon\n",
    "    end\n",
    "    return splits\n",
    "end\n",
    "\n",
    "nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eef4d41",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6ad8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "function cyclical_encoding(z,time, branch)\n",
    "    # Encode the d_k values\n",
    "    a = (branch == 1) ? findfirst(time .>= 24*365 ) : findfirst(time .>= 24 )# this should be 1 year\n",
    "    dsin = sin.( 2π .* (z .- 1) ./ a)\n",
    "    dcos = cos.( 2π .* (z .- 1) ./ a)\n",
    "    return Float32.(vcat(dsin' , dcos'))      # date d_t = [ dsin, dcos ]\n",
    "end \n",
    "\n",
    "function make_seq2seq(branch; hidden_size = 64, alt = 0)\n",
    "    # Creating the layers\n",
    "    encoder = LSTM(3 => hidden_size)\n",
    "    \n",
    "    # Checking which inputs go in the decoder\n",
    "    # alt=0 means the standard decoder with only the dates\n",
    "    # alt=1 is the first alternative both forcing and dates\n",
    "    # alt=2 is the seconf alternative with only the forcing\n",
    "\n",
    "    n1 = (alt <= 1) ? 2 : 0                 # nonzero if we are using date in decoder \n",
    "    n2 = (alt >= 1) ? (3 - branch) : 0        # nonzero if we are using forcing in decoder\n",
    "    n_in = n1 + n2\n",
    "    decoder = LSTM(n_in => hidden_size)\n",
    "    outlayer = Dense(hidden_size => branch)\n",
    "\n",
    "    # Move model to gpu or keep on cpu\n",
    "    if use_gpu\n",
    "        encoder, decoder, outlayer = gpu.( (encoder, decoder, outlayer) )\n",
    "    end\n",
    "    \n",
    "    return Chain(encoder, decoder, outlayer)\n",
    "end\n",
    "\n",
    "\n",
    "function forward_pass(m, x_seq, decod_seq)\n",
    "    # Forward pass of the model, corresponds to the seq2seq architecture\n",
    "    encoder, decoder, outlayer = m[1], m[2], m[3]\n",
    "    # Reset hidden/cell states memory (no leakage)\n",
    "    Flux.reset!(encoder); Flux.reset!(decoder)\n",
    "    # Encode internal h_t and c_t states using prev values\n",
    "    x_seq = reshape(x_seq, size(x_seq, 1), 1, size(x_seq, 2))  \n",
    "    encoder(x_seq)\n",
    "    # Initialize with encoder states\n",
    "    decoder.state = encoder.state\n",
    "    decod_seq = reshape(decod_seq, size(decod_seq,1), 1, size(decod_seq,2))\n",
    "    # Forecast the next steps  \n",
    "    yhat = outlayer(decoder(decod_seq))\n",
    "    return dropdims(yhat; dims=2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69002e70",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71b8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_rolling_tscv(m_tuple, X, Y, time, alt, branch; horizon=30, n_epochs = 1000, decay_rate=0.99, step=5)\n",
    "    # Create container\n",
    "    loss_epoch = []\n",
    "\n",
    "    # Create folds\n",
    "    splits = make_rolling_test_splits(time, branch; horizon=horizon) \n",
    "\n",
    "    # Initialize optimizer and scheduler\n",
    "    learning_rate = 0.001 / (decay_rate^(n_epochs/step))\n",
    "    optimizer = Flux.setup(Adam(learning_rate), m_tuple)\n",
    "    lr_scheduler = Stateful(Step(learning_rate, decay_rate, step))\n",
    "    # Train data\n",
    "    # @showprogress \n",
    "    for epoch in 1:n_epochs\n",
    "        # Perform one epoch of training\n",
    "        for (fold, (train_idx, val_idx)) in enumerate(splits)\n",
    "            # Form training and validation sets\n",
    "            x_train = X[:, train_idx] \n",
    "            y_val = Y[:, val_idx]\n",
    "            # Encode dates and forcing            \n",
    "            date_seq = cyclical_encoding(val_idx, time, branch)\n",
    "            fork = X[1+branch:end, val_idx]\n",
    "            decod_seq = (date_seq, vcat(date_seq, fork), fork)[alt + 1]\n",
    "            \n",
    "            loss, grads = Flux.withgradient(m_tuple) do m \n",
    "                # Evaluate model and loss inside gradient context:\n",
    "                y_hat = forward_pass(m, x_train, decod_seq) # apply the model to the input batch      \n",
    "                Flux.huber_loss(y_hat, y_val) # compute the loss\n",
    "            end\n",
    "            Flux.update!(optimizer, m_tuple, grads[1])\n",
    "            if epoch == n_epochs \n",
    "                y_hat = forward_pass(m_tuple, x_train, decod_seq)\n",
    "                push!(loss_epoch, Flux.huber_loss(y_hat, y_val)  )\n",
    "            end\n",
    "        end\n",
    "        # Update learning rate\n",
    "        nextlr = ParameterSchedulers.next!(lr_scheduler)\n",
    "        Optimisers.adjust!(optimizer, nextlr)\n",
    "    end\n",
    "    loss_stat = mean(loss_epoch), std(loss_epoch)\n",
    "\n",
    "    return m_tuple, loss_stat\n",
    "end\n",
    "\n",
    "nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8674b1b8",
   "metadata": {},
   "source": [
    "# Unroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58fe0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unroll the model to get the output for the initial condition\n",
    "function unroll(model, X, Y, horizon, branch, time, alt; forcing = false)    \n",
    "    # Create container\n",
    "    loss_epoch = []\n",
    "    markers = [] # for plotting\n",
    "\n",
    "    # Create folds\n",
    "    splits = make_rolling_test_splits(time, branch; horizon=horizon)\n",
    "\n",
    "    lookback = (branch == 1) ? findfirst(time .>= 24*365 ) : findfirst(time .>= 24 )\n",
    "    X1 = nothing; temp = nothing\n",
    "\n",
    "    n_steps = Int(floor( (size(time,1) - lookback )/ horizon))\n",
    "\n",
    "    outputs = zeros(Float32, branch, n_steps*horizon)\n",
    "    # Unroll the model for n_step\n",
    "    for (fold, (train_idx, val_idx)) in enumerate(splits)\n",
    "        # Form training and validation sets\n",
    "        X1 = (forcing || fold==1) ? X[:, train_idx] :  hcat(X1, temp)\n",
    "        y_val = Y[:, val_idx]\n",
    "        # Encode dates and forcing            \n",
    "        date_seq = cyclical_encoding(val_idx, time, branch)\n",
    "        fork = X[1+branch:end, val_idx]\n",
    "        decod_seq = (date_seq, vcat(date_seq, fork), fork)[alt + 1]\n",
    "\n",
    "        y = forward_pass(model, X1, decod_seq)  \n",
    "        # Compare forecast to exact value  \n",
    "        push!(loss_epoch, Flux.huber_loss(y, y_val)  )\n",
    "        # For plotting\n",
    "        push!(markers, val_idx[1])\n",
    "        # Add to unroll array\n",
    "        outputs[:,mod.(val_idx, lookback) .+ lookback.*( val_idx .÷ lookback .- 1)] = y    \n",
    "        # Add the unroll to the training data     \n",
    "        temp = vcat( y , X[1+branch:end,val_idx]) \n",
    "    end\n",
    "    loss_stat = mean(loss_epoch), std(loss_epoch)\n",
    "    return outputs, markers, loss_stat\n",
    "end\n",
    "\n",
    "\n",
    "function plot_unroll(Y_unroll, Y, time, branch; markers = nothing)\n",
    "    # Plot the unroll\n",
    "    lookback = (branch == 1) ? findfirst(time .>= 24*365 ) : findfirst(time .>= 24 )\n",
    "    \n",
    "    if branch == 1\n",
    "        p1=plot(time[lookback:lookback+size(Y_unroll,2)-1]/24, Y_unroll[1,:], xticks = 0:60:maximum(time), xlabel=\"time [days]\", label=\"ML unroll\", legend=true, title=\"Mean water level in lake\")# ,st=:scatter)\n",
    "        plot!(p1, time[lookback:end]/24, Y[1,lookback:end], xlabel=\"time [days]\", label=\"Ground truth\", legend=true, title=\"Mean water level in lake\")\n",
    "        if markers !== nothing\n",
    "            scatter!(p1, [time[markers]/24], Y_unroll[1,mod.(markers,lookback)], markershape=:x, label=\"\", markersize = 5, markerstrokewidth = 2)\n",
    "        end\n",
    "        compareplot1 = plot(p1,layout=(1,1), size=(1200,300))\n",
    "        display(compareplot1)\n",
    "\n",
    "    elseif branch == 2\n",
    "        p2=plot(time[lookback:lookback+size(Y_unroll,2)-1], Y_unroll[1,:], xlabel=\"time [hours]\", xticks = 0:5:maximum(time), label=\"ML unroll\", legend=true, title=\"Surface slope in lake [m]\")\n",
    "        plot!(p2, time[lookback:end], Y[1,lookback:end], xlabel=\"time [hours]\", label=\"Ground truth\", legend=true, title=\"Surface slope in lake [m]\")\n",
    "      \n",
    "        p3=plot(time[lookback:lookback+size(Y_unroll,2)-1], Y_unroll[2,:], xlabel=\"time [hours]\", xticks = 0:5:maximum(time), label=\"ML unroll\", legend=true, title=\"Velocity in lake [m/s]\")\n",
    "        plot!(p3, time[lookback:end], Y[2,lookback:end], xlabel=\"time [hours]\", label=\"Ground truth\", legend=true, title=\"Velocity in lake [m/s]\")\n",
    "        \n",
    "        if markers !== nothing\n",
    "            scatter!(p2, [time[markers]], Y_unroll[1,mod.(markers,lookback)], markershape=:x, label=\"\", markersize = 5, markerstrokewidth = 2)\n",
    "            scatter!(p3, [time[markers]], Y_unroll[2,mod.(markers,lookback)], markershape=:x, label=\"\", markersize = 5, markerstrokewidth = 2)\n",
    "        end\n",
    "\n",
    "        compareplot2 = plot(p2,p3,layout=(2,1), size=(1200,600))\n",
    "        display(compareplot2)\n",
    "    end\n",
    "end\n",
    "\n",
    "nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42453f56",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c32a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load(\"Dataset/model_0d_lake_sep.jld2\")\n",
    "branch = 2\n",
    "horizon = 40\n",
    "alt = 0\n",
    "X, Y, time = collect_data(data, branch, noise_param = 0.1)  \n",
    "m_tuple = make_seq2seq(branch,alt = alt)\n",
    "m_tuple, loss_stat = train_rolling_tscv(m_tuple, X, Y, time, alt, branch; horizon=horizon)\n",
    "horizon_u = horizon\n",
    "Y_unroll, markers, loss_stat_u = unroll(m_tuple, X, Y, horizon_u, branch, time, alt)\n",
    "@show loss_stat_u , loss_stat\n",
    "plot_unroll(Y_unroll, Y, time, branch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc7a0d5",
   "metadata": {},
   "source": [
    "# Comparing alts\n",
    "\n",
    "Compare the unrolling loss of the model on the storm dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addfe4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hyperparameters\n",
    "# branch = 1\n",
    "\n",
    "# loss = Dict{Tuple{Int,Int,Int,Int}, Any}()\n",
    "\n",
    "# for horizon in [60,50,40,30,20]\n",
    "#     for alt in [0,1,2]\n",
    "#         for epochs in [100, 200, 300, 400]\n",
    "\n",
    "#             @show horizon, alt, epochs\n",
    "\n",
    "#             horizon_u = horizon\n",
    "\n",
    "#             # --- Training data ---\n",
    "#             data = load(\"Dataset/model_0d_lake_sep.jld2\")\n",
    "#             X, Y, time = collect_data(data, branch, noise_param = 0.1)\n",
    "\n",
    "#             # --- Build model ---\n",
    "#             m_tuple = make_seq2seq(branch, alt = alt)\n",
    "\n",
    "#             # --- Train model ---\n",
    "#             m_tuple, loss_stat = train_rolling_tscv(m_tuple, X, Y, time, alt, branch; horizon=horizon, n_epochs=epochs)\n",
    "\n",
    "#             # --- Test / unroll on odd data ---\n",
    "#             for storm in [1,2,3]\n",
    "#                 if storm == 1\n",
    "#                     data2 = load(\"Dataset/model_0d_lake_sep_odd1.jld2\")\n",
    "#                 elseif storm == 2\n",
    "#                     data2 = load(\"Dataset/model_0d_lake_sep_odd2.jld2\")\n",
    "#                 elseif storm == 3\n",
    "#                     data2 = load(\"Dataset/model_0d_lake_sep_odd2.jld2\")\n",
    "#                 end \n",
    "#                 X, Y, time = collect_data(data2, branch, noise_param = 0.1)\n",
    "\n",
    "#                 Y_unroll, markers, loss_stat_u = unroll(m_tuple, X, Y, horizon_u, branch, time, alt)\n",
    "\n",
    "#                 # --- Save loss in dictionary ---\n",
    "#                 loss[(horizon, alt, epochs, storm)] = loss_stat_u\n",
    "#             end\n",
    "\n",
    "#         end\n",
    "#     end\n",
    "# end\n",
    "\n",
    "\n",
    "# @save \"loss_alt_comp_storms_b1.jld2\" loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2db283",
   "metadata": {},
   "source": [
    "# Plotting the unrolling of Alts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fed114",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data=load(\"Dataset/model_0d_lake_sep.jld2\")\n",
    "# branch = 2\n",
    "# horizon = 40\n",
    "# horizon_u = horizon\n",
    "\n",
    "# p1=plot();p2=plot(); p3=plot()\n",
    "\n",
    "# X, Y, time = collect_data(data, branch, noise_param = 0.1)  \n",
    "\n",
    "# for alt in [0,1,2]\n",
    "#     m_tuple = make_seq2seq(branch,alt = alt)\n",
    "#     m_tuple, loss_stat = train_rolling_tscv(m_tuple, X, Y, time, alt, branch; horizon=horizon)    \n",
    "#     @show loss_stat, alt\n",
    "#     Y_unroll, markers, loss_stat_u = unroll(m_tuple, X, Y, horizon_u, branch, time, alt)\n",
    "#     @show loss_stat_u, alt\n",
    "\n",
    "#     lookback = (branch == 1) ? findfirst(time .>= 24*365 ) : findfirst(time .>= 24 )\n",
    "\n",
    "#     if branch == 1\n",
    "#         plot!(p1,time[lookback:lookback+size(Y_unroll,2)-1]/24, Y_unroll[1,:], xticks = 0:60:maximum(time), xlabel=\"time [days]\", label=\"ML unroll\", legend=true, title=\"Mean water level in lake\")# ,st=:scatter)\n",
    "#         if alt==2\n",
    "#             plot!(p1, time[lookback:end]/24, Y[1,lookback:end], xlabel=\"time [days]\", label=\"Ground truth\", legend=true, title=\"Mean water level in lake\")\n",
    "#         end\n",
    "\n",
    "#     elseif branch == 2\n",
    "#         plot!(p2,time[lookback:lookback+size(Y_unroll,2)-1], Y_unroll[1,:], xlabel=\"time [hours]\", xticks = 0:5:maximum(time), label=\"ML unroll alt=$alt\", legend=true, title=\"Surface slope in lake [m]\")\n",
    "#         plot!(p3,time[lookback:lookback+size(Y_unroll,2)-1], Y_unroll[2,:], xlabel=\"time [hours]\", xticks = 0:5:maximum(time), label=\"ML unroll alt=$alt\", legend=true, title=\"Velocity in lake [m/s]\")\n",
    "#         if alt==2\n",
    "#             plot!(p2, time[lookback:end], Y[1,lookback:end], xlabel=\"time [hours]\", label=\"Ground truth\", legend=true, title=\"Surface slope in lake [m]\")\n",
    "#             plot!(p3, time[lookback:end], Y[2,lookback:end], xlabel=\"time [hours]\", label=\"Ground truth\", legend=true, title=\"Velocity in lake [m/s]\")\n",
    "#         end\n",
    "#     end\n",
    "\n",
    "# end\n",
    "\n",
    "\n",
    "# compareplot2 = plot(p2,p3,layout=(2,1), size=(1200,600))\n",
    "# # compareplot2 = plot(p1,layout=(1,1), size=(1200,300))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8bb45c",
   "metadata": {},
   "source": [
    "# Creating data for plots\n",
    "\n",
    "This code creates data for plots made in File \"Make plots.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b134b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load(\"Dataset/model_0d_lake_sep.jld2\")\n",
    "\n",
    "# # Prepare a container to store results\n",
    "# # results[branch][horizon_t][epochs][alt][horizon_u] => loss_stat_u\n",
    "# results = Dict{Int, Any}()\n",
    "\n",
    "# # train_results[branch][horizon_t][epochs][alt] => Dict:loss_stat_t => ..., :train_time  => ...)\n",
    "# train_results = Dict{Int, Any}()\n",
    "\n",
    "# for branch in [2]\n",
    "#     X, Y, time = collect_data(data, branch, noise_param = 0.1)\n",
    "#     branch_results = Dict{Int, Dict{Int, Dict{Int, Dict{Int, Any}}}}() \n",
    "#     # branch_results[horizon_t][epochs][alt][horizon_u] => loss_stat_u\n",
    "#     branch_train= Dict{Int, Dict{Int, Dict{Int, Any}}}()\n",
    "#     # branch_train[horizon_t][epochs][alt] => Dict:loss_stat_t => ..., :train_time  => ...)\n",
    "\n",
    "#     for horizon_t in [60,40,20,10,5,1]\n",
    "#         splits = make_rolling_test_splits(time, branch; horizon=horizon_t)\n",
    "#         branch_results[horizon_t] = Dict{Int, Dict{Int, Dict{Int, Any}}}()\n",
    "#         branch_train[horizon_t]   = Dict{Int, Dict{Int, Any}}()\n",
    "\n",
    "#         for epochs in [100, 200, 300, 400]\n",
    "#             branch_results[horizon_t][epochs] = Dict{Int, Dict{Int, Any}}()\n",
    "#             branch_train[horizon_t][epochs]   = Dict{Int, Any}()\n",
    "\n",
    "#             for alt in [0,1,2]\n",
    "#                 m_tuple = make_seq2seq(branch, alt=alt)\n",
    "#                 train_time = @elapsed begin\n",
    "#                     m_tuple, loss_stat_t = train_rolling_tscv(m_tuple, X, Y, time, alt, branch;\n",
    "#                                                                 horizon=horizon_t, n_epochs=epochs)\n",
    "#                 end  # timed\n",
    "#                 horizon_u_results = Dict{Int, Any}()\n",
    "#                 branch_train[horizon_t][epochs][alt] = Dict(\n",
    "#                     :loss_stat_t => loss_stat_t,\n",
    "#                     :train_time  => train_time,\n",
    "#                 )\n",
    "\n",
    "#                 for horizon_u in [60,40,20,10,5,1]\n",
    "#                     Y_unroll, markers, loss_stat_u = unroll(m_tuple, X, Y, horizon_u, branch, time, alt)\n",
    "#                     horizon_u_results[horizon_u] = loss_stat_u\n",
    "#                 end  # horizon_u\n",
    "\n",
    "#                 branch_results[horizon_t][epochs][alt] = horizon_u_results\n",
    "#                 @save \"checkpoint_b2.jld2\" branch_results branch_train\n",
    "#                 print(\"\\rTraining branch $branch, horizon $horizon_t, epochs $epochs, alt $alt, train_time $train_time\")\n",
    "#                 flush(stdout)\n",
    "#             end #alt\n",
    "#         end # epochs\n",
    "#     end # horizon_t\n",
    "#     results[branch] = branch_results\n",
    "#     train_results[branch] = branch_train\n",
    "#     @save \"checkpoint2_b2.jld2\" results train_results\n",
    "\n",
    "# end # branch\n",
    "\n",
    "# @save \"final_res_b2.jld2\" results train_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
